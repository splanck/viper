//===----------------------------------------------------------------------===//
//
// Part of the Viper project, under the MIT License.
// See LICENSE for license information.
//
//===----------------------------------------------------------------------===//
//
// File: src/codegen/aarch64/LowerILToMIR.cpp
// Purpose: Minimal ILâ†’MIR lowering adapter for AArch64 (Phase A).
//
//===----------------------------------------------------------------------===//

#include "LowerILToMIR.hpp"

#include "il/core/Instr.hpp"
#include "il/core/Type.hpp"

namespace viper::codegen::aarch64
{
namespace
{
using il::core::Opcode;

static const char *condForOpcode(Opcode op)
{
    switch (op)
    {
        case Opcode::ICmpEq: return "eq";
        case Opcode::ICmpNe: return "ne";
        case Opcode::SCmpLT: return "lt";
        case Opcode::SCmpLE: return "le";
        case Opcode::SCmpGT: return "gt";
        case Opcode::SCmpGE: return "ge";
        case Opcode::UCmpLT: return "lo";
        case Opcode::UCmpLE: return "ls";
        case Opcode::UCmpGT: return "hi";
        case Opcode::UCmpGE: return "hs";
        default: return nullptr;
    }
}

static int indexOfParam(const il::core::BasicBlock &bb, unsigned tempId)
{
    for (size_t i = 0; i < bb.params.size(); ++i)
        if (bb.params[i].id == tempId) return static_cast<int>(i);
    return -1;
}

} // namespace

MFunction LowerILToMIR::lowerFunction(const il::core::Function &fn) const
{
    MFunction mf{};
    mf.name = fn.name;
    mf.blocks.emplace_back();
    auto &bbMir = mf.blocks.back();

    if (fn.retType.kind != il::core::Type::Kind::I64)
        return mf; // Only i64 patterns supported in this phase

    const auto &argOrder = ti_->intArgOrder;

    if (!fn.blocks.empty())
    {
        const auto &bb = fn.blocks.front();

        // ret %paramN fast-path
        if (fn.blocks.size() == 1 && !bb.instructions.empty() && !bb.params.empty())
        {
            const auto &retI = bb.instructions.back();
            if (retI.op == il::core::Opcode::Ret && !retI.operands.empty())
            {
                const auto &rv = retI.operands[0];
                if (rv.kind == il::core::Value::Kind::Temp)
                {
                    int pIdx = indexOfParam(bb, rv.id);
                    if (pIdx >= 0)
                    {
                        const PhysReg src = argOrder[static_cast<size_t>(pIdx)];
                        if (src != PhysReg::X0)
                            bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(src)}});
                        return mf;
                    }
                }
            }
        }

        // rr ops on entry params feeding ret
        if (fn.blocks.size() == 1 && bb.instructions.size() >= 2 && bb.params.size() >= 2)
        {
            const auto &opI = bb.instructions[bb.instructions.size() - 2];
            const auto &retI = bb.instructions.back();
            if ((opI.op == il::core::Opcode::Add || opI.op == il::core::Opcode::IAddOvf ||
                 opI.op == il::core::Opcode::Sub || opI.op == il::core::Opcode::ISubOvf ||
                 opI.op == il::core::Opcode::Mul || opI.op == il::core::Opcode::IMulOvf ||
                 opI.op == il::core::Opcode::And || opI.op == il::core::Opcode::Or ||
                 opI.op == il::core::Opcode::Xor ||
                 opI.op == il::core::Opcode::ICmpEq || opI.op == il::core::Opcode::ICmpNe ||
                 opI.op == il::core::Opcode::SCmpLT || opI.op == il::core::Opcode::SCmpLE ||
                 opI.op == il::core::Opcode::SCmpGT || opI.op == il::core::Opcode::SCmpGE ||
                 opI.op == il::core::Opcode::UCmpLT || opI.op == il::core::Opcode::UCmpLE ||
                 opI.op == il::core::Opcode::UCmpGT || opI.op == il::core::Opcode::UCmpGE) &&
                retI.op == il::core::Opcode::Ret && opI.result && !retI.operands.empty())
            {
                const auto &retV = retI.operands[0];
                if (retV.kind == il::core::Value::Kind::Temp && retV.id == *opI.result &&
                    opI.operands.size() == 2 && opI.operands[0].kind == il::core::Value::Kind::Temp &&
                    opI.operands[1].kind == il::core::Value::Kind::Temp)
                {
                    const int idx0 = indexOfParam(bb, opI.operands[0].id);
                    const int idx1 = indexOfParam(bb, opI.operands[1].id);
                    if (idx0 >= 0 && idx1 >= 0 && idx0 < 8 && idx1 < 8)
                    {
                        const PhysReg src0 = argOrder[static_cast<size_t>(idx0)];
                        const PhysReg src1 = argOrder[static_cast<size_t>(idx1)];
                        // Normalize to x0,x1 using x9 scratch
                        bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X9), MOperand::regOp(src1)}});
                        bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(src0)}});
                        bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X1), MOperand::regOp(PhysReg::X9)}});
                        switch (opI.op)
                        {
                            case Opcode::Add:
                            case Opcode::IAddOvf:
                                bbMir.instrs.push_back(MInstr{MOpcode::AddRRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X1)}});
                                break;
                            case Opcode::Sub:
                            case Opcode::ISubOvf:
                                bbMir.instrs.push_back(MInstr{MOpcode::SubRRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X1)}});
                                break;
                            case Opcode::Mul:
                            case Opcode::IMulOvf:
                                bbMir.instrs.push_back(MInstr{MOpcode::MulRRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X1)}});
                                break;
                            case Opcode::And:
                                bbMir.instrs.push_back(MInstr{MOpcode::AddRRR /* placeholder? */}); // will be overridden by emitter mapping
                                bbMir.instrs.back().opc = MOpcode::AddRRR; // keep structure consistent
                                // but we don't have AND in MIR; use emitter helper via direct mapping not supported here
                                // fall-through to emulate using emitter RR ops is handled via CLI path; omit in MIR here
                                break;
                            case Opcode::Or:
                            case Opcode::Xor:
                                // Not yet in MIR minimal; handled by CLI path (kept unchanged)
                                break;
                            case Opcode::ICmpEq: case Opcode::ICmpNe:
                            case Opcode::SCmpLT: case Opcode::SCmpLE: case Opcode::SCmpGT: case Opcode::SCmpGE:
                            case Opcode::UCmpLT: case Opcode::UCmpLE: case Opcode::UCmpGT: case Opcode::UCmpGE:
                                bbMir.instrs.push_back(MInstr{MOpcode::CmpRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X1)}});
                                bbMir.instrs.push_back(MInstr{MOpcode::Cset, {MOperand::regOp(PhysReg::X0), MOperand::condOp(condForOpcode(opI.op))}});
                                break;
                            default: break;
                        }
                        return mf;
                    }
                }
            }
        }

        // ri/shift-imm and immediate compares
        if (fn.blocks.size() == 1 && bb.instructions.size() >= 2 && !bb.params.empty())
        {
            const auto &binI = bb.instructions[bb.instructions.size() - 2];
            const auto &retI = bb.instructions.back();
            const bool isAdd = (binI.op == il::core::Opcode::Add || binI.op == il::core::Opcode::IAddOvf);
            const bool isSub = (binI.op == il::core::Opcode::Sub || binI.op == il::core::Opcode::ISubOvf);
            const bool isShl = (binI.op == il::core::Opcode::Shl);
            const bool isLShr = (binI.op == il::core::Opcode::LShr);
            const bool isAShr = (binI.op == il::core::Opcode::AShr);
            const bool isICmpImm = (condForOpcode(binI.op) != nullptr);
            if ((isAdd || isSub || isShl || isLShr || isAShr) && retI.op == il::core::Opcode::Ret && binI.result &&
                !retI.operands.empty() && binI.operands.size() == 2)
            {
                const auto &retV = retI.operands[0];
                if (retV.kind == il::core::Value::Kind::Temp && retV.id == *binI.result)
                {
                    const auto &o0 = binI.operands[0];
                    const auto &o1 = binI.operands[1];
                    auto emitImm = [&](unsigned paramIndex, long long imm)
                    {
                        const PhysReg src = argOrder[paramIndex];
                        bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(src)}});
                        if (isAdd) bbMir.instrs.push_back(MInstr{MOpcode::AddRI, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        else if (isSub) bbMir.instrs.push_back(MInstr{MOpcode::SubRI, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        else if (isShl) bbMir.instrs.push_back(MInstr{MOpcode::LslRI, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        else if (isLShr) bbMir.instrs.push_back(MInstr{MOpcode::LsrRI, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        else if (isAShr) bbMir.instrs.push_back(MInstr{MOpcode::AsrRI, {MOperand::regOp(PhysReg::X0), MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                    };
                    if (o0.kind == il::core::Value::Kind::Temp && o1.kind == il::core::Value::Kind::ConstInt)
                    {
                        for (size_t i = 0; i < bb.params.size(); ++i)
                            if (bb.params[i].id == o0.id && i < 8) { emitImm(static_cast<unsigned>(i), o1.i64); return mf; }
                    }
                    if (o1.kind == il::core::Value::Kind::Temp && o0.kind == il::core::Value::Kind::ConstInt)
                    {
                        if (isAdd || isShl || isLShr || isAShr)
                        {
                            for (size_t i = 0; i < bb.params.size(); ++i)
                                if (bb.params[i].id == o1.id && i < 8) { emitImm(static_cast<unsigned>(i), o0.i64); return mf; }
                        }
                    }
                }
            }
            if (isICmpImm && retI.op == il::core::Opcode::Ret && binI.result && !retI.operands.empty() &&
                binI.operands.size() == 2)
            {
                const auto &retV = retI.operands[0];
                if (retV.kind == il::core::Value::Kind::Temp && retV.id == *binI.result)
                {
                    const auto &o0 = binI.operands[0];
                    const auto &o1 = binI.operands[1];
                    auto emitCmpImm = [&](unsigned paramIndex, long long imm)
                    {
                        const PhysReg src = argOrder[paramIndex];
                        if (src != PhysReg::X0)
                            bbMir.instrs.push_back(MInstr{MOpcode::MovRR, {MOperand::regOp(PhysReg::X0), MOperand::regOp(src)}});
                        bbMir.instrs.push_back(MInstr{MOpcode::CmpRI, {MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        bbMir.instrs.push_back(MInstr{MOpcode::Cset, {MOperand::regOp(PhysReg::X0), MOperand::condOp(condForOpcode(binI.op))}});
                    };
                    if (o0.kind == il::core::Value::Kind::Temp && o1.kind == il::core::Value::Kind::ConstInt)
                    {
                        for (size_t i = 0; i < bb.params.size(); ++i)
                            if (bb.params[i].id == o0.id && i < 8) { emitCmpImm(static_cast<unsigned>(i), o1.i64); return mf; }
                    }
                }
            }
        }

        // ret const i64 anywhere in the function
        for (const auto &block : fn.blocks)
        {
            if (!block.instructions.empty())
            {
                const auto &term = block.instructions.back();
                if (term.op == il::core::Opcode::Ret && !term.operands.empty())
                {
                    const auto &v = term.operands[0];
                    if (v.kind == il::core::Value::Kind::ConstInt)
                    {
                        const long long imm = v.i64;
                        // Prefer movz/movk path in AsmEmitter for wide values.
                        bbMir.instrs.push_back(MInstr{MOpcode::MovRI, {MOperand::regOp(PhysReg::X0), MOperand::immOp(imm)}});
                        return mf;
                    }
                }
            }
        }
    }
    return mf;
}

} // namespace viper::codegen::aarch64

