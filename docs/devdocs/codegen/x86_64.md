# Phase A Codegen Quickstart (x86-64 SysV)

## ABI snapshot

- **Calling convention:** System V AMD64.
- **Argument passing:**
    - Integer/pointer: RDI, RSI, RDX, RCX, R8, R9 then stack (8-byte slots).
    - Floating-point: XMM0–XMM7 then stack (16-byte shadow per spill).
    - Structs up to 16 bytes split across GPR/SIMD per ABI classes; larger by pointer.
- **Return values:**
    - Integer/pointer/scalar ≤64 bits in RAX (plus RDX if >64 bits but ≤128).
    - Float/vector in XMM0 (XMM1 if second part).
    - Aggregates >128 bits: caller allocates and passes pointer in RDI; callee writes there.
- **Register preservation:**
    - Caller-saved: RAX, RCX, RDX, RSI, RDI, R8–R11, XMM0–XMM15.
    - Callee-saved: RBX, RBP, R12–R15 (plus stack pointer RSP). Preserve XMM callee-saves only if ABI extension
      requires (Phase A: none).

## i1 lowering policy

- Represent boolean SSA values as 0/1 in 64-bit GPRs.
- On calls/returns, widen to full 8-byte slot (zero-extend) before passing or returning.
- Memory slots for `i1` still occupy 1 byte; extend on load, truncate on store.

## Stack discipline

- Maintain 16-byte alignment at every call boundary. Adjust prologue to align `RSP` after pushing the return address.
- Spill slots allocated in 16-byte multiples when mixing SIMD/GPR spills; pure GPR spills may use 8-byte stride, but
  ensure call sites realign.

## Block parameters to ParallelCopy

- IL block parameters become edge-specific ParallelCopy bundles in MIR. Each CFG edge owns one copy list executed before
  the successor block executes.
- Ensure φ-like moves are ordered with ParallelCopy to avoid clobbers; insert temps when cycles exist.

## Phase A scope

- Implements instruction selection to MIR, linear scan register allocation, frame layout, and textual assembly
  emission (`asmText`).
- Defers: stack-based argument expansion beyond simple scalars, vector ABI oddities (mask regs, varargs), shrink
  wrapping, pro/epilogue peepholes, ELF emission, debug info.

## Phase A completeness

- [x] Return moves handled via tail ParallelCopy folding.
- [x] `select` lowering for GPR and f64 operands.
- [x] Shift family (`shl`, `lshr`, `ashr`) mapped to x86-64 semantics.
- [x] f64 constant materialization through literal pools or inline immediates.
- [x] String literal plumbing from IL globals through data segments.
- [x] Call-site alignment checks enforcing 16-byte `RSP` invariants.

## Operation notes

- **Signed & unsigned div/rem:** `LowerDiv.cpp` expands MIR pseudos into guarded `idiv`/`div` sequences and emits
  `rt_trap_div0` calls when the divisor is zero.
- **Bitwise ops:** `LowerILToMIR::lowerBinary` selects `AND`/`OR`/`XOR` for both register-register and
  register-immediate forms.

## Adding a new opcode

1. Introduce the MIR opcode in `src/codegen/x86_64/MachineIR.hpp`/`.cpp`, documenting operands, result class, and
   verifier hooks.
   Update any helpers or enumerations that surface the opcode to the rest of the backend.
2. Extend IL → MIR selection in `src/codegen/x86_64/LowerILToMIR.hpp`/`.cpp` (and add a focused matcher in `ISel.hpp`/
   `.cpp` if needed).
   Keep selection predicates narrow so unrelated patterns are unaffected.
3. Map the opcode to a physical mnemonic inside `src/codegen/x86_64/AsmEmitter.hpp`/`.cpp`, keeping operand ordering
   consistent.
   Verify operand modifiers (e.g., `byte ptr`) match the intended encoding.
4. Update lowering helpers or post-processing passes (e.g., `LowerDiv.cpp` for div/rem expansions) if the opcode needs
   multi-instruction support.
5. Update the selection tests (e.g., `src/tests/codegen/x86_64/*`) with a focused case that exercises the new path
   end-to-end.
   Aim for a MIR dump and final assembly snippet to catch regressions early.
6. Regenerate or adjust any MIR/assembly golden files touched by the opcode, keeping Phase A invariants intact.
   Confirm stack alignment assertions still pass under `ctest`.

## Backend pipeline map

1. **IL to MIR lowering** (`LowerILToMIR::lower`, `LowerILToMIR.hpp`/`.cpp`): pattern-match IL ops to target MIR. Add
   new opcode translations here.
2. **Parallel copy resolution** (`ParallelCopyResolver.hpp`): expand block parameter moves into ordered copies during
   lowering.
3. **Instruction selection helpers** (`ISel.hpp`/`.cpp`): rewrite generic MIR pseudos into target-specific ops (
   arith/compare/select).
4. **Division expansion** (`LowerDiv.cpp`): rewrite signed/unsigned div/rem pseudos into guarded IDIV/DIV sequences.
5. **Register allocation** (`RegAllocLinear.hpp`/`.cpp`): map virtual regs to physical; insert spills/reloads.
6. **Frame/stack planning** (`FrameLowering.hpp`/`.cpp`): assign spill slots and emit prologue/epilogue.
7. **Post-RA cleanup** (`Peephole.hpp`/`.cpp`): peephole optimizations applied after register allocation.
8. **Emitter** (`AsmEmitter.hpp`/`.cpp`): produce final assembly text and manage rodata pools.

- To add a new lowering pass, slot it before RA if it rewrites SSA, after RA otherwise; update `Backend.cpp`'s pipeline
  to invoke it.

## Debugging checklist

- Inspect emitted assembly by reading `asmText` from the compiled function object.
- Use targeted `dbg()` prints inside lowering stages; guard them with compile-time flags to keep release builds quiet.
- Check MIR output at each pipeline stage to trace lowering/allocation decisions.

