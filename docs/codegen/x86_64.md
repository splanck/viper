# Phase A Codegen Quickstart (x86-64 SysV)

## ABI snapshot
- **Calling convention:** System V AMD64.
- **Argument passing:**
  - Integer/pointer: RDI, RSI, RDX, RCX, R8, R9 then stack (8-byte slots).
  - Floating-point: XMM0–XMM7 then stack (16-byte shadow per spill).
  - Structs up to 16 bytes split across GPR/SIMD per ABI classes; larger by pointer.
- **Return values:**
  - Integer/pointer/scalar ≤64 bits in RAX (plus RDX if >64 bits but ≤128).
  - Float/vector in XMM0 (XMM1 if second part).
  - Aggregates >128 bits: caller allocates and passes pointer in RDI; callee writes there.
- **Register preservation:**
  - Caller-saved: RAX, RCX, RDX, RSI, RDI, R8–R11, XMM0–XMM15.
  - Callee-saved: RBX, RBP, R12–R15 (plus stack pointer RSP). Preserve XMM callee-saves only if ABI extension requires (Phase A: none).

## i1 lowering policy
- Represent boolean SSA values as 0/1 in 64-bit GPRs.
- On calls/returns, widen to full 8-byte slot (zero-extend) before passing or returning.
- Memory slots for `i1` still occupy 1 byte; extend on load, truncate on store.

## Stack discipline
- Maintain 16-byte alignment at every call boundary. Adjust prologue to align `RSP` after pushing the return address.
- Spill slots allocated in 16-byte multiples when mixing SIMD/GPR spills; pure GPR spills may use 8-byte stride, but ensure call sites realign.

## Block parameters to ParallelCopy
- IL block parameters become edge-specific ParallelCopy bundles in MIR. Each CFG edge owns one copy list executed before the successor block executes.
- Ensure φ-like moves are ordered with ParallelCopy to avoid clobbers; insert temps when cycles exist.

## Phase A scope
- Implements instruction selection to MIR, linear scan register allocation, frame layout, and textual assembly emission (`asmText`).
- Defers: stack-based argument expansion beyond simple scalars, vector ABI oddities (mask regs, varargs), shrink wrapping, pro/epilogue peepholes, ELF emission, debug info.

## Phase A completeness
- [x] Return moves handled via tail ParallelCopy folding.
- [x] `select` lowering for GPR and f64 operands.
- [x] Shift family (`shl`, `lshr`, `ashr`) mapped to x86-64 semantics.
- [x] f64 constant materialization through literal pools or inline immediates.
- [x] String literal plumbing from IL globals through data segments.
- [x] Call-site alignment checks enforcing 16-byte `RSP` invariants.

## Operation notes
- **Signed & unsigned div/rem:** `LowerDiv.cpp` expands MIR pseudos into guarded `idiv`/`div` sequences and emits `rt_trap_div0` calls when the divisor is zero.
- **Bitwise ops:** `LowerILToMIR::lowerBinary` selects `AND`/`OR`/`XOR` for both register-register and register-immediate forms.

## Adding a new opcode
1. Introduce the MIR opcode in `src/codegen/x86_64/MachineIR.hpp`/`.cpp`, documenting operands, result class, and verifier hooks.
   Update any helpers or enumerations that surface the opcode to the rest of the backend.
2. Extend IL → MIR selection in `src/codegen/x86_64/LowerILToMIR.hpp`/`.cpp` (and add a focused matcher in `ISel.hpp`/`.cpp` if needed).
   Keep selection predicates narrow so unrelated patterns are unaffected.
3. Map the opcode to a physical mnemonic inside `src/codegen/x86_64/AsmEmitter.hpp`/`.cpp`, keeping operand ordering consistent.
   Verify operand modifiers (e.g., `byte ptr`) match the intended encoding.
4. Update lowering helpers or post-processing passes (e.g., `LowerDiv.cpp` for div/rem expansions) if the opcode needs multi-instruction support.
5. Update the selection tests (e.g., `tests/codegen/x86_64/*`) with a focused case that exercises the new path end-to-end.
   Aim for a MIR dump and final assembly snippet to catch regressions early.
6. Regenerate or adjust any MIR/assembly golden files touched by the opcode, keeping Phase A invariants intact.
   Confirm stack alignment assertions still pass under `ctest`.

## Backend pipeline map
1. **IL to MIR lowering** (`LowerILToMIR::lower`, `LowerILToMIR.hpp`/`.cpp`): pattern-match IL ops to target MIR. Add new opcode translations here.
2. **Parallel copy resolution** (`ParallelCopyResolver.hpp`): expand block parameter moves into ordered copies during lowering.
3. **Instruction selection helpers** (`ISel.hpp`/`.cpp`): rewrite generic MIR pseudos into target-specific ops (arith/compare/select).
4. **Division expansion** (`LowerDiv.cpp`): rewrite signed/unsigned div/rem pseudos into guarded IDIV/DIV sequences.
5. **Register allocation** (`RegAllocLinear.hpp`/`.cpp`): map virtual regs to physical; insert spills/reloads.
6. **Frame/stack planning** (`FrameLowering.hpp`/`.cpp`): assign spill slots and emit prologue/epilogue.
7. **Post-RA cleanup** (`Peephole.hpp`/`.cpp`): optional cleanups gated by `--enable-peepholes`.
8. **Emitter** (`AsmEmitter.hpp`/`.cpp`): produce final assembly text and manage rodata pools.
- To add a new lowering pass, slot it before RA if it rewrites SSA, after RA otherwise; update `Backend.cpp`'s pipeline to invoke it.

## Debugging checklist
- Inspect emitted assembly with `--dump-asm` or by reading `asmText` from the compiled function object.
- Enable peephole optimizations (`--enable-peepholes`) to catch missed canonicalizations.
- Build with `VIPER_MIR_DEBUG=1` (or equivalent flag) to dump intermediate MIR, including ParallelCopies and allocation decisions.
- Use targeted `dbg()` prints inside lowering stages; guard them with compile-time flags to keep release builds quiet.

