# Phase A Codegen Quickstart (x86-64 SysV)

## ABI snapshot
- **Calling convention:** System V AMD64.
- **Argument passing:**
  - Integer/pointer: RDI, RSI, RDX, RCX, R8, R9 then stack (8-byte slots).
  - Floating-point: XMM0–XMM7 then stack (16-byte shadow per spill).
  - Structs up to 16 bytes split across GPR/SIMD per ABI classes; larger by pointer.
- **Return values:**
  - Integer/pointer/scalar ≤64 bits in RAX (plus RDX if >64 bits but ≤128).
  - Float/vector in XMM0 (XMM1 if second part).
  - Aggregates >128 bits: caller allocates and passes pointer in RDI; callee writes there.
- **Register preservation:**
  - Caller-saved: RAX, RCX, RDX, RSI, RDI, R8–R11, XMM0–XMM15.
  - Callee-saved: RBX, RBP, R12–R15 (plus stack pointer RSP). Preserve XMM callee-saves only if ABI extension requires (Phase A: none).

## i1 lowering policy
- Represent boolean SSA values as 0/1 in 64-bit GPRs.
- On calls/returns, widen to full 8-byte slot (zero-extend) before passing or returning.
- Memory slots for `i1` still occupy 1 byte; extend on load, truncate on store.

## Stack discipline
- Maintain 16-byte alignment at every call boundary. Adjust prologue to align `RSP` after pushing the return address.
- Spill slots allocated in 16-byte multiples when mixing SIMD/GPR spills; pure GPR spills may use 8-byte stride, but ensure call sites realign.

## Block parameters to ParallelCopy
- IL block parameters become edge-specific ParallelCopy bundles in MIR. Each CFG edge owns one copy list executed before the successor block executes.
- Ensure φ-like moves are ordered with ParallelCopy to avoid clobbers; insert temps when cycles exist.

## Phase A scope
- Implements instruction selection to MIR, linear scan register allocation, frame layout, and textual assembly emission (`asmText`).
- Defers: stack-based argument expansion beyond simple scalars, vector ABI oddities (mask regs, varargs), shrink wrapping, pro/epilogue peepholes, ELF emission, debug info.

## Phase A completeness
- [x] Return moves handled via tail ParallelCopy folding.
- [x] `select` lowering for GPR and f64 operands.
- [x] Shift family (`shl`, `lshr`, `ashr`) mapped to x86-64 semantics.
- [x] f64 constant materialization through literal pools or inline immediates.
- [x] String literal plumbing from IL globals through data segments.
- [x] Call-site alignment checks enforcing 16-byte `RSP` invariants.

## Adding a new opcode
1. Introduce the MIR opcode in `include/mir/opcodes.def`, documenting operands and result class.
   Note the expected register banks or immediates to guide lowering and validation.
2. Extend the IL → MIR matcher in `src/codegen/x86_64/Lowering.cpp` to emit the new opcode for matching IL instructions.
   Keep selection predicates narrow so unrelated patterns are unaffected.
3. Map the opcode to a physical mnemonic inside `src/codegen/x86_64/AsmEmitter.cpp`, keeping operand ordering consistent.
   Verify operand modifiers (e.g., `byte ptr`) match the intended encoding.
4. Update the selection tests (e.g., `tests/codegen/x86_64/*`) with a focused case that exercises the new path end-to-end.
   Aim for a MIR dump and final assembly snippet to catch regressions early.
5. Regenerate or adjust any MIR/assembly golden files touched by the opcode, keeping Phase A invariants intact.
   Confirm stack alignment assertions still pass under `ctest`.

## Backend pipeline map
1. **IL to MIR lowering** (`lowerIlFunction`): pattern-match IL ops to target MIR. Add new opcode translations here.
2. **SSA utilities** (`buildParallelCopies`, `fixPhiEdges`): handle block params → ParallelCopy. Extend when new SSA forms appear.
3. **Frame/stack planning** (`assignFrameSlots`, `finalizePrologueEpilogue`): choose spill slots, align stack.
4. **Register allocation** (`linearScan`): map virtual regs to physical; insert spills/reloads.
5. **Post-RA cleanup & peepholes** (`runPeepholes`, `simplifyCopies`): optional; ensure `--enable-peepholes` flag controls extras.
6. **Emitter** (`emitAsmText`): produce final text; consult this when debugging.
- To add a new lowering pass, slot it before RA if it rewrites SSA, after RA otherwise; update `runBackendPipeline` to invoke it.

## Debugging checklist
- Inspect emitted assembly with `--dump-asm` or by reading `asmText` from the compiled function object.
- Enable peephole optimizations (`--enable-peepholes`) to catch missed canonicalizations.
- Build with `VIPER_MIR_DEBUG=1` (or equivalent flag) to dump intermediate MIR, including ParallelCopies and allocation decisions.
- Use targeted `dbg()` prints inside lowering stages; guard them with compile-time flags to keep release builds quiet.

